{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "F01C08 Corretor Ortográfico em Python: Aplicando técnicas de NLP",
      "provenance": [],
      "collapsed_sections": [
        "Qk3V2Bw-CeIL",
        "SSFgBqnIOT5D",
        "Tqc8EJ7YPUNC",
        "9SQVj9Shdiiy",
        "xf59vzhoeAN6",
        "Auw0_f-FecCA",
        "8n_j2DWWB770",
        "Vq5eqOrUCWMx",
        "1rxdVXnTje0A",
        "mcT7I5MV8azZ",
        "1lIZ9qHw9hnG",
        "Ur_FLKmb9inm",
        "R47ZabiT9jG_",
        "bV8QDcbN9jbY",
        "-m582bq19jst",
        "X4cidb2W9j-9",
        "8WQpQFoV9kQn"
      ],
      "authorship_tag": "ABX9TyPVO1wLLHJSMpKgRujI5tXd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jhonoratoc/cursos-alura/blob/main/F01C08_Corretor_Ortogr%C3%A1fico_em_Python_Aplicando_t%C3%A9cnicas_de_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qk3V2Bw-CeIL"
      },
      "source": [
        "# <font color=navyblue> `AULA 01` Explorando um projeto de NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b42qEI-PEahW"
      },
      "source": [
        "## Resumo e documentação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NowYqfQ3vGk0"
      },
      "source": [
        "### Resumo da aula 01"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBjUcHEXEhw-"
      },
      "source": [
        "* O que é a NLP?  (+ exemplos)\n",
        "* Como um corretor ortográfico funciona?\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "* Criando o notebook\n",
        "* Importando base de dados com open() e salvando em uma variável (artigos)\n",
        "\n",
        "---\n",
        "\n",
        "* O que é corpus? O que é token?\n",
        "* Descobrindo a quantidade de tokens do corpus usando o método .split()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9ePAjOKvQpK"
      },
      "source": [
        "### Documentação da aula 01"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjAFizkEEheM"
      },
      "source": [
        "[...]\n",
        "\n",
        "---\n",
        "* [*open()*](https://docs.python.org/3/library/functions.html#open)\n",
        "* [*.read()*](https://www.w3schools.com/python/ref_file_read.asp)\n",
        "\n",
        "---\n",
        "* [*.split()*](https://www.w3schools.com/python/ref_string_split.asp)\n",
        "* [*len()*](https://www.w3schools.com/python/ref_func_len.asp)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4xnqfzRCeIL"
      },
      "source": [
        "## 1) Explorando o problema"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgoKH-PDJfX4"
      },
      "source": [
        "### O que é a NLP?\n",
        "\n",
        "Não existe, de forma natural (digitando, escrevendo, falando), um tipo de comunicação direta Homem -> Máquina // Máquina -> Homem.\n",
        "\n",
        "É necessária a existência de um **intermediário** para que a comunicação seja fluida e **permita que o homem entenda a máquina e a máquina entenda o homem**. Esse intermediário é o NLP.\n",
        "\n",
        "Aplicações: assistentes virtuais, análise de sentimento, plataformas tradutoras, buscadores, corretores ortográficos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbcu7Ex8KByn"
      },
      "source": [
        "### Como um corretor ortográfico funciona?\n",
        "\n",
        "Palavra digitada de forma incorreta --> NLP trabalha como intermediário, identifica o erro e recomenda a palavra digitada da forma correta\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0jsgw6HCeIM"
      },
      "source": [
        "## 2) Importando um corpus textual"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKTfGn5LM9ff"
      },
      "source": [
        "### O que é um corpus?\n",
        "É um conjunto de documentos. No nosso caso, é um conjunto de artigos do blog da Alura."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGRPoUd4CeIM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b7a31c1-6f73-446c-ca9d-76480005096f"
      },
      "source": [
        "## importando base de dados fornecidas pelo curso\n",
        "## para abrir um arquivo de texto no python, usaremos a built-in function open()\n",
        "## https://docs.python.org/3/library/functions.html#open\n",
        "\n",
        "## para ler de fato o arquivo txt, é necessário seguir esse processo:\n",
        "\n",
        "with open('artigos.txt', mode='r') as f:   ## mode='r' --> modo de leitura\n",
        "  artigos = f.read()\n",
        "\n",
        "print(artigos[:500])  ## print apenas os primeiros 500 caracteres"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "imagem \n",
            "\n",
            "Temos a seguinte classe que representa um usuário no nosso sistema:\n",
            "\n",
            "java\n",
            "\n",
            "Para salvar um novo usuário, várias validações são feitas, como por exemplo: Ver se o nome só contém letras, [**o CPF só números**] e ver se o usuário possui no mínimo 18 anos. Veja o método que faz essa validação:\n",
            "\n",
            "java \n",
            "\n",
            "Suponha agora que eu tenha outra classe, a classe `Produto`, que contém um atributo nome e eu quero fazer a mesma validação que fiz para o nome do usuário: Ver se só contém letras. E aí? Vou\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIWZ815KPzjY"
      },
      "source": [
        "## 3) Tokenização"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "929TTL_6CeIM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe7494e6-5ad7-4847-d5df-632dbfafbd94"
      },
      "source": [
        "## será que esse corpus possui dimensões suficientes para ser usado na criação de um corretor?\n",
        "## quanto mais palavras, mais repertório para corrigir uma palavra\n",
        "\n",
        "## quantas palavras existem no nosso corpus?\n",
        "## se eu usar len(artigos), o valor retornado será o de caracteres, consideravelmente maior do que o número de palavras\n",
        "\n",
        "len(artigos)  ## 2,6 milhões de palavras? não"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2605046"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dgk9Dh20CeIN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5033ce3e-0803-40e1-c2c9-9048ebda420d"
      },
      "source": [
        "## uma opção a ser utilizada é a funcão .split(), que divide uma string dividida por um determinado separador\n",
        "## vamos mostrar um exemplo\n",
        "\n",
        "txt_exemplo = 'Olá, tudo bem?'\n",
        "tokens = txt_exemplo.split()   ## separando o string acima\n",
        "\n",
        "tokens  ## nota-se que o método usou os espaços em branco como separador e não levou em consideração os sinais de pontuação (vírgula e interrogação)\n",
        "        ## mesmo não separando as palavras do jeito gramaticalmente correto, podemos usar essa divisão do split\n",
        "        ## cada item dessa lista é um token"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Olá,', 'tudo', 'bem?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IRlYTrMdAKx"
      },
      "source": [
        "## como separar uma string em palavras desconsiderando os sinais de pontuação?\n",
        "## melhor dizendo, como refinar a tokenização?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSFgBqnIOT5D"
      },
      "source": [
        "# <font color=navyblue> `AULA 02` Utilizando NLTK para tokenizar um texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14I7zPh_FQJJ"
      },
      "source": [
        "## Resumo e documentação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1Ac5LgfvdIe"
      },
      "source": [
        "### Resumo da aula 02"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MA0sbmDFQJO"
      },
      "source": [
        "* Refinando a tokenização (incluindo sinais de pontuação na separação dos tokens) usando nltk.tokenize.word_tokenize\n",
        "\n",
        "---\n",
        "\n",
        "* Criando uma função para extrair apenas as palavras da lista de tokens utilizando o método .isalpha()\n",
        "\n",
        "---\n",
        "\n",
        "* Aplicando a tokenização com nltk + função para extrair apenas as palavas para descobrir a quantidade de palavras do corpus textual que foi importado\n",
        "\n",
        "---\n",
        "\n",
        "* Criando uma função para que todas as palavras da lista fiquem apenas com letras minúsculas, com o objetivo de uniformizar e normalizar a lista\n",
        "\n",
        "---\n",
        "\n",
        "* Usando a função set(lista_palavras) para excluir as duplicatas da lista, reduzindo de 393 mil para 17 mil palavras contidas no corretor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QL3H2l3vdq6"
      },
      "source": [
        "### Documentação da aula 02"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtV15BK7FQJQ"
      },
      "source": [
        "\n",
        "* [*nltk.tokenize.word_tokenize*](https://www.nltk.org/api/nltk.tokenize.html?highlight=word_tokenize#nltk.tokenize.word_tokenize)\n",
        "\n",
        "---\n",
        "\n",
        "* [*.isalpha()*](https://www.w3schools.com/python/ref_string_isalpha.asp)\n",
        "\n",
        "---\n",
        "\n",
        "[...]\n",
        "\n",
        "---\n",
        "\n",
        "* [*lower()*](https://www.w3schools.com/python/ref_string_lower.asp)\n",
        "\n",
        "---\n",
        "\n",
        "* [*set()*](https://www.geeksforgeeks.org/python-set-method/)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3XCGzhkO7qB"
      },
      "source": [
        "## 1) Refinando a tokenização"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2RFKfy0e-WF"
      },
      "source": [
        "O processo de transformar um arquivo de texto em pequenos tokens é chamado de Tokenização, o qual é recorrente no pré-processamento ou na análise estatística de dados textuais.\n",
        "\n",
        "Por ser bastante utilizado, temos ferramentas que facilitam a sua implementação. Uma delas é o nltk (*Natural Language Tool Kit* -- https://www.nltk.org/), que será usada para refinar a tokenização, considerando os sinais de pontuação na divisão."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DAhMSFAe6o7",
        "outputId": "aa1734ee-728f-47c0-f58b-0243f96bf461"
      },
      "source": [
        "## faremos uma versão mais refinada da tokenização usando o nltk\n",
        "## para isso, importaremos o nltk e baixaremos a fonte de dados 'punkt'\n",
        "\n",
        "\n",
        "import nltk                    ## importando o nltk\n",
        "nltk.download('punkt')         ## baixando 'punkt'\n",
        "\n",
        "## utilizaremos a variável txt_exemplo para dividir os tokens, levando em conta a pontuação\n",
        "## o resultado será guardado na variável txt_separado, utilizada anteriormente\n",
        "\n",
        "txt_separado = nltk.tokenize.word_tokenize(txt_exemplo)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xJtjZZre6mQ",
        "outputId": "c52c8913-fe33-4bf0-9096-8ce793f931c9"
      },
      "source": [
        "txt_separado  ## a versão do texto tokenizado, considerando os espaços em branco e todas as pontuações"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Olá', ',', 'tudo', 'bem', '?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMJ_vQOAPHQy"
      },
      "source": [
        "## 2) Separando palavras de tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sx3-mD5Wiem0",
        "outputId": "ee3737ab-b213-4772-9eec-b856872353e5"
      },
      "source": [
        "## se eu fizer um len em txt_separado, quantos itens a função identifica?\n",
        "\n",
        "len(txt_separado)  ## cinco; mas são três palavas, correto?"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qy-Skbn6iekb",
        "outputId": "97566af0-aaf6-43ab-c6a9-27bec6a142d7"
      },
      "source": [
        "## como separar sinais de pontuação e palavras dentro da lista de tokens?\n",
        "## criemos uma função para fazer essa separação\n",
        "\n",
        "## existe um método de strings chamado .isalpha(), que identifica se uma string contém apenas letras do alfabeto\n",
        "\n",
        "def separa_palavras(lista_tokens):\n",
        "  lista_palavras = []\n",
        "  for token in lista_tokens:\n",
        "    if token.isalpha():\n",
        "      lista_palavras.append(token)\n",
        "  return lista_palavras\n",
        "\n",
        "## dados de entrada: tokens com sinais de pontuação\n",
        "separa_palavras(txt_separado)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Olá', 'tudo', 'bem']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEfXoLDscCLc"
      },
      "source": [
        "## 3) Contando palavras do Corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33FWB4cPcXmV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae316072-0da3-4483-9291-b1e5e8021d87"
      },
      "source": [
        "## deu certo!!\n",
        "## agora faremos o mesmo procedimento para o corpus textual importado\n",
        "\n",
        "lista_tokens = nltk.tokenize.word_tokenize(artigos)             ## tokenização com nltk\n",
        "lista_palavras = separa_palavras(lista_tokens)                  ## filtragem dos tokens usando a função\n",
        "print(f'O arquivo possui {len(lista_palavras)} palavras')       ## quantas palavras possuem o corpus?"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "O arquivo possui 393914 palavras\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghJ1VcjXEyQ8"
      },
      "source": [
        "## 4) Normalização"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hxvmxxdto5oK",
        "outputId": "ed3e797a-09be-42cc-83c3-0152ba828b6d"
      },
      "source": [
        "## quantas palavras o corretor será capaz de corrigir? 393914?\n",
        "## esse valor compreende as duplicatas de cada palavra\n",
        "\n",
        "print(lista_palavras[:5])\n",
        "\n",
        "## nota-se que algumas palavras começam com letra maiúscula\n",
        "## quando comparadas com suas correspondentes que começam com letra minúscula, o corretor considerará que são palavras diferentes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['imagem', 'Temos', 'a', 'seguinte', 'classe']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWS1--CHEyQ9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1f8ae87-738b-4e0b-9607-02bf81d3673f"
      },
      "source": [
        "## para que isso não aconteça, devemos normalizar a nossa lista de palavras, uniformizando e trocando todas as letras maiúsculas por letras minúsculas\n",
        "## usaremos o método lower(), que transforma todas as letras maiúsculas em minúsculas\n",
        "## para agilizar o processo, criaremos uma função que varrerá toda a lista de palavras do nosso corpus\n",
        "\n",
        "\n",
        "def normalizacao(lista_palavras):\n",
        "  lista_normalizada = []\n",
        "  for i in lista_palavras:\n",
        "    lista_normalizada.append(i.lower())\n",
        "  return lista_normalizada\n",
        "\n",
        "lista_normalizada = normalizacao(lista_palavras)\n",
        "print(lista_normalizada[:5])\n",
        "\n",
        "## funcionou!!"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['imagem', 'temos', 'a', 'seguinte', 'classe']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWbNwrbEE2Y8"
      },
      "source": [
        "## 5) Tipos de palavras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GaFIii6E2Y9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6072f2a6-1873-4842-d768-20b9bf263ea7"
      },
      "source": [
        "## agora que já temos nossa lista normalizada, poderemos remover as repetições de palavras\n",
        "## existem algumas formas para fazermos isso\n",
        "## usaremos a função set() para retornar apenas os valores únicos de uma lista\n",
        "## https://www.geeksforgeeks.org/python-set-method/\n",
        "\n",
        "len(set(lista_normalizada))\n",
        "\n",
        "## 17 mil palavras únicas"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17652"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tqc8EJ7YPUNC"
      },
      "source": [
        "# <font color=navyblue> `AULA 03` Desenvolvendo e testando o corretor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZ4QWFMovukK"
      },
      "source": [
        "## Resumo e documentação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PysZ3nBEvukM"
      },
      "source": [
        "### Resumo da aula 03"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8Twrwytvy88"
      },
      "source": [
        "* Explicações sobre o funcionamento do corretor\n",
        "\n",
        "---\n",
        "\n",
        "* Criação da função responsável por fatiar as palavras **gerador_palavras()**, que depois será complementada pela função de inserir letras após o fatiamento\n",
        "\n",
        "---\n",
        "\n",
        "* Criação da função de inserção das letras pós fatiamento **insere_letras()**, que gerará uma lista de palavras criadas com base na palavra original (escrita de forma errada)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "* Início da construção da função **corretor()**, que identificará qual palavra da lista de palavras geradas corresponde à palavra original\n",
        "* Para identificar a palavra, será construída uma função para determinar a probabilidade daquele elemento da lista de palavras ser a palavra correta\n",
        "\n",
        "---\n",
        "\n",
        "* Cálculo da **probabilidade()** da palavra corrigida corresponder à palavra correta usando nltk.FreqDist(lista_normalizada) / len(lista_normalizada)\n",
        "* Teste da função corretor + função probabilidade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vV3RTUrKvukS"
      },
      "source": [
        "### Documentação da aula 03"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkD2vc0Vvzqz"
      },
      "source": [
        "* [...]\n",
        "\n",
        "---\n",
        "\n",
        "* [...]\n",
        "\n",
        "---\n",
        "\n",
        "* [...]\n",
        "\n",
        "---\n",
        "\n",
        "* [*max()*](https://www.w3schools.com/python/ref_func_max.asp)\n",
        "\n",
        "---\n",
        "\n",
        "* [*nltk.FreqDist*](https://tedboy.github.io/nlps/generated/generated/nltk.FreqDist.html)\n",
        "* [*nltk.FreqDist.most_common*](https://tedboy.github.io/nlps/generated/generated/nltk.FreqDist.most_common.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5duc-rAaccD4"
      },
      "source": [
        "## 1) Detalhando o corretor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CYVN8KUsLn4"
      },
      "source": [
        "Queremos que o corretor receba uma palavra incorreta (lgica, por exemplo) e devolva a palavra correta (lógica). Para isso, a NLP precisa intermediar essa transformação.\n",
        "\n",
        "Devemos gerar novas palavras a partir da palavra errada (lgica -- lógica com uma letra a menos); uma das palavras geradas deve ser a correta (lógica) e certamente está contida na nossa lista de palavras do nosso corpus textual.\n",
        "\n",
        "Como fazer essa transição entre lgica e lógica?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGWcCm6SvMWM"
      },
      "source": [
        "1) Gerar novas palavras a partir da palavra errada\n",
        "\n",
        "Nesse caso, o erro da palavra é ter uma letra a menos. Nosso modelo de NLP deve formar novas palavras inserindo uma letra na palavra escrita erroneamente.\n",
        "\n",
        "\n",
        "Faremos uma tabela com três colunas:\n",
        "* letras: todas as opções de letras do alfabeto a serem inseridas\n",
        "* operação: operação de inserção da letra -- em que parte da palavra será inserida a letra?\n",
        "* resultado: palavras formadas após a inserção da letra\n",
        "\n",
        "\n",
        "Sobre a parte da operação: dividiremos as palavras em duas partes -- o lado esquerdo e o lado direito. A letra será inserida entre as duas partes.\n",
        "\n",
        "*esquerdo + LETRA + direito = palavra*\n",
        "\n",
        "Mas em qual parte da palavra haverá essa divisão?\n",
        "* null + LETRA + lgica\n",
        "* l + LETRA + gica\n",
        "* lg + LETRA + ica\n",
        "* lgi + LETRA + ca\n",
        "* lgic + LETRA + a\n",
        "* lgica + LETRA + null\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBXIit4cEtBY"
      },
      "source": [
        "## 2) Fatiando strings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkJFuIQN4ZdP",
        "outputId": "c7895e87-ad99-45d4-bb60-879b7841699f"
      },
      "source": [
        "lista = 'lgica'         ## demonstrando o fatiamento de strings\n",
        "(lista[:0], lista[0:])  ## lado esquerdo e direito de cada string"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('', 'lgica')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "fUOsKytSxaFP",
        "outputId": "f422e7b6-b8f1-4be4-cf52-0508e6e95f76"
      },
      "source": [
        "## façamos a função para gerar palavras a partir da palavra errada\n",
        "  ## dividir palavras\n",
        "  ## inserir letras\n",
        "  ## juntar tudo\n",
        "  ## guardar na lista de palavras\n",
        "\n",
        "\n",
        "'''palavra_exemplo = 'lgica'\n",
        "\n",
        "def gerador_palavras(palavra):                        ## criando a função\n",
        "  fatias = []\n",
        "  for i in range(len(palavra)+1):                     ## criando um for para fatias as palavras em duas partes; lembrar de somar +1 pra pegar todos os fatiamentos\n",
        "    fatias.append((palavra[:i],palavra[i:]))          ## cada fatiamento vira uma tupla com as duas partes da palavra dividida -- dando append das tuplas na lista fatias\n",
        "  palavras_geradas = insere_letras(fatias)            ## as palavras geradas virão de outra função, que inserirá letras nas fatias da esquerda e da direita da palavra\n",
        "  return palavras_geradas'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"palavra_exemplo = 'lgica'\\n\\ndef gerador_palavras(palavra):                        ## criando a função\\n  fatias = []\\n  for i in range(len(palavra)+1):                     ## criando um for para fatias as palavras em duas partes; lembrar de somar +1 pra pegar todos os fatiamentos\\n    fatias.append((palavra[:i],palavra[i:]))          ## cada fatiamento vira uma tupla com as duas partes da palavra dividida -- dando append das tuplas na lista fatias\\n  palavras_geradas = insere_letras(fatias)            ## as palavras geradas virão de outra função, que inserirá letras nas fatias da esquerda e da direita da palavra\\n  return palavras_geradas\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdAfWX9PDrY-"
      },
      "source": [
        "## 3) Operação de inserção"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXHUgloNxaCu",
        "outputId": "cec4e8e5-8420-4834-8a69-2c1fc76d5ac4"
      },
      "source": [
        "## criemos a função insere_letras(fatias)\n",
        "\n",
        "palavra_exemplo = 'lgica'\n",
        "\n",
        "def insere_letras(fatias):\n",
        "  novas_palavras = []\n",
        "  letras = 'abcdefghijklmnopqrstuvwxyzàáâãèéêìíîòóôõùúûç'         ## todas as letras\n",
        "  for E,D in fatias:                                              ## para cada lado esquerdo e direito das minhas palavras\n",
        "    for letra in letras:                                          ## para cada letra a ser inserida\n",
        "      novas_palavras.append(E + letra + D)                        ## esquerdo + letra + direito = palavra nova\n",
        "  return novas_palavras\n",
        "\n",
        "def gerador_palavras(palavra):                        ## criando a função\n",
        "  fatias = []\n",
        "  for i in range(len(palavra)+1):                     ## criando um for para fatiar as palavras em duas partes; lembrar de somar +1 pra pegar todos os fatiamentos\n",
        "    fatias.append((palavra[:i],palavra[i:]))          ## cada fatiamento vira uma tupla com as duas partes da palavra dividida -- dando append das tuplas na lista fatias\n",
        "  palavras_geradas = insere_letras(fatias)            ## as palavras geradas virão de outra função, que inserirá letras nas fatias da esquerda e da direita da palavra\n",
        "  return palavras_geradas\n",
        "\n",
        "\n",
        "print(gerador_palavras(palavra_exemplo))              ## a lista de palavras que podem ser feitas inserindo uma letra"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['algica', 'blgica', 'clgica', 'dlgica', 'elgica', 'flgica', 'glgica', 'hlgica', 'ilgica', 'jlgica', 'klgica', 'llgica', 'mlgica', 'nlgica', 'olgica', 'plgica', 'qlgica', 'rlgica', 'slgica', 'tlgica', 'ulgica', 'vlgica', 'wlgica', 'xlgica', 'ylgica', 'zlgica', 'àlgica', 'álgica', 'âlgica', 'ãlgica', 'èlgica', 'élgica', 'êlgica', 'ìlgica', 'ílgica', 'îlgica', 'òlgica', 'ólgica', 'ôlgica', 'õlgica', 'ùlgica', 'úlgica', 'ûlgica', 'çlgica', 'lagica', 'lbgica', 'lcgica', 'ldgica', 'legica', 'lfgica', 'lggica', 'lhgica', 'ligica', 'ljgica', 'lkgica', 'llgica', 'lmgica', 'lngica', 'logica', 'lpgica', 'lqgica', 'lrgica', 'lsgica', 'ltgica', 'lugica', 'lvgica', 'lwgica', 'lxgica', 'lygica', 'lzgica', 'làgica', 'lágica', 'lâgica', 'lãgica', 'lègica', 'légica', 'lêgica', 'lìgica', 'lígica', 'lîgica', 'lògica', 'lógica', 'lôgica', 'lõgica', 'lùgica', 'lúgica', 'lûgica', 'lçgica', 'lgaica', 'lgbica', 'lgcica', 'lgdica', 'lgeica', 'lgfica', 'lggica', 'lghica', 'lgiica', 'lgjica', 'lgkica', 'lglica', 'lgmica', 'lgnica', 'lgoica', 'lgpica', 'lgqica', 'lgrica', 'lgsica', 'lgtica', 'lguica', 'lgvica', 'lgwica', 'lgxica', 'lgyica', 'lgzica', 'lgàica', 'lgáica', 'lgâica', 'lgãica', 'lgèica', 'lgéica', 'lgêica', 'lgìica', 'lgíica', 'lgîica', 'lgòica', 'lgóica', 'lgôica', 'lgõica', 'lgùica', 'lgúica', 'lgûica', 'lgçica', 'lgiaca', 'lgibca', 'lgicca', 'lgidca', 'lgieca', 'lgifca', 'lgigca', 'lgihca', 'lgiica', 'lgijca', 'lgikca', 'lgilca', 'lgimca', 'lginca', 'lgioca', 'lgipca', 'lgiqca', 'lgirca', 'lgisca', 'lgitca', 'lgiuca', 'lgivca', 'lgiwca', 'lgixca', 'lgiyca', 'lgizca', 'lgiàca', 'lgiáca', 'lgiâca', 'lgiãca', 'lgièca', 'lgiéca', 'lgiêca', 'lgiìca', 'lgiíca', 'lgiîca', 'lgiòca', 'lgióca', 'lgiôca', 'lgiõca', 'lgiùca', 'lgiúca', 'lgiûca', 'lgiçca', 'lgicaa', 'lgicba', 'lgicca', 'lgicda', 'lgicea', 'lgicfa', 'lgicga', 'lgicha', 'lgicia', 'lgicja', 'lgicka', 'lgicla', 'lgicma', 'lgicna', 'lgicoa', 'lgicpa', 'lgicqa', 'lgicra', 'lgicsa', 'lgicta', 'lgicua', 'lgicva', 'lgicwa', 'lgicxa', 'lgicya', 'lgicza', 'lgicàa', 'lgicáa', 'lgicâa', 'lgicãa', 'lgicèa', 'lgicéa', 'lgicêa', 'lgicìa', 'lgicía', 'lgicîa', 'lgicòa', 'lgicóa', 'lgicôa', 'lgicõa', 'lgicùa', 'lgicúa', 'lgicûa', 'lgicça', 'lgicaa', 'lgicab', 'lgicac', 'lgicad', 'lgicae', 'lgicaf', 'lgicag', 'lgicah', 'lgicai', 'lgicaj', 'lgicak', 'lgical', 'lgicam', 'lgican', 'lgicao', 'lgicap', 'lgicaq', 'lgicar', 'lgicas', 'lgicat', 'lgicau', 'lgicav', 'lgicaw', 'lgicax', 'lgicay', 'lgicaz', 'lgicaà', 'lgicaá', 'lgicaâ', 'lgicaã', 'lgicaè', 'lgicaé', 'lgicaê', 'lgicaì', 'lgicaí', 'lgicaî', 'lgicaò', 'lgicaó', 'lgicaô', 'lgicaõ', 'lgicaù', 'lgicaú', 'lgicaû', 'lgicaç']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZXybEzJFf9L"
      },
      "source": [
        "## 4) Construindo a função corretor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KLpRsZvEuve"
      },
      "source": [
        "## a partir de uma palavra com uma letra faltando, conseguimos gerar uma lista de palavras apenas adicionando uma letra (de uma lista de letras) em diversas posições\n",
        "## uma delas é a palavra correta; como identificá-la?\n",
        "## construiremos a função corretor para que essa identificação seja automática\n",
        "\n",
        "def corretor(palavra):                                                ## o corretor vai receber a palavra escrita de forma errada\n",
        "  palavras_geradas = gerador_palavras(palavra)                        ## usando a função gerador_palavras, geraremos a lista de palavras\n",
        "  palavra_correta = max(palavras_geradas, key=probabilidade)          ## a palavra correta será aquela com maior probabilidade de ser, por isso o uso da função max(palavras_geradas, key=probabilidade)\n",
        "                                                                      ## esse valor de probabilidade será determinado por uma função chamada probabilidade, que será criada no próximo vídeo\n",
        "  return palavra_correta\n",
        "\n",
        "## esse é o início da construção da função corretor -- no vídeo seguinte construiremos a função probabilidade e voltaremos a montar a função corretor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4U07E7sFf92"
      },
      "source": [
        "## 5) Probabilidade das palavras geradas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LnWCanaEutJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fe7ee50-bfb6-4b7a-8bae-8f82690a6339"
      },
      "source": [
        "## criando a função probabilidade\n",
        "\n",
        "## como calcular a probabilidade da palavra estar correta?\n",
        "## prob = (frequência que a palavra aparece no corpus)/(nº total de palavras do corpus)\n",
        "\n",
        "## para isso, usaremos novamente o nltk\n",
        "  ## nltk.FreqDist https://tedboy.github.io/nlps/generated/generated/nltk.FreqDist.html --------------------------> calculando a frequência de cada palavra na lista_normalizada\n",
        "  ## nltk.FreqDist.most_common https://tedboy.github.io/nlps/generated/generated/nltk.FreqDist.most_common.html --> listando os itens mais frequentes em lista_normalizada\n",
        "\n",
        "frequencia = nltk.FreqDist(lista_normalizada)\n",
        "frequencia.most_common(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('de', 15494),\n",
              " ('o', 13966),\n",
              " ('que', 12225),\n",
              " ('a', 11034),\n",
              " ('e', 10478),\n",
              " ('para', 7694),\n",
              " ('um', 6346),\n",
              " ('é', 5881),\n",
              " ('uma', 5202),\n",
              " ('do', 5116)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EElmsnUEuq2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46ba0008-637e-427b-dc72-4d27b30ff9c9"
      },
      "source": [
        "## para saber a frequência de cada palavra, podemos buscar como se fosse uma dicionário\n",
        "\n",
        "frequencia['lógica']  ## lógica aparece 87 vezes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "87"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPd8n2ZgdBGu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0132a748-85a0-4e24-be84-7d8d57728920"
      },
      "source": [
        "## a função probabilidade vai varrer todas as palavras geradas a partir da palavra errada\n",
        "## e calcular a probabilidade da palavra estar correta com base no banco de palavras em lista_normalizada\n",
        "\n",
        "def probabilidade(palavra_gerada):\n",
        "  return frequencia[palavra_gerada]/len(lista_normalizada)\n",
        "\n",
        "probabilidade('lógica')   ## bem pequena, né? mas ainda é maior do que a probabilidade das outras palavras geradas"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.00022086039084673304"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JM4BYt2dBEB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b2f5b2b9-12c7-48dc-b9d3-30568096f71c"
      },
      "source": [
        "## hora de testar\n",
        "\n",
        "corretor(palavra_exemplo)  ## palavra_exemplo é 'lgica'\n",
        "\n",
        "## funcionou!!"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'lógica'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9GgEwqpGk9N9",
        "outputId": "a5f00978-6c56-413f-d6ec-500e038d6333"
      },
      "source": [
        "## exemplos:\n",
        "\n",
        "corretor('uando')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'quando'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "98HrriymlJ2p",
        "outputId": "0baa58b7-4908-474a-ffbf-bcb9359508bc"
      },
      "source": [
        "corretor('sndo')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'sendo'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "D6MqBUhHlJ0J",
        "outputId": "d0ced815-ec53-46b5-aad3-6194b98d29b4"
      },
      "source": [
        "corretor('diícil')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'difícil'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "VJx_dISUk9D3",
        "outputId": "248d0c83-86c9-4858-fbb3-5882977359fe"
      },
      "source": [
        "corretor('tete')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'teste'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lhHvXhOkhrl"
      },
      "source": [
        "Essa foi a correção de apenas um tipo de erro, causado quando esquecemos de digitar uma letra da palavra. Existem muitos outros tipos, que ainda serão abordados mais tarde neste curso."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SQVj9Shdiiy"
      },
      "source": [
        "# <font color=navyblue> `AULA 04` Avaliando a qualidade do corretor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3Y5RbhBv5DP"
      },
      "source": [
        "## Resumo e documentação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFkybUaAv5DQ"
      },
      "source": [
        "### Resumo da aula 04"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "te9kWhx6v5DQ"
      },
      "source": [
        "* Como avaliar a taxa de acerto do corretor no estado atual?\n",
        "* Criando a função **cria_dados_teste(nome_arquivo)** para criar a base de dados que avaliará o corretor, utilizando um arquivo txt (palavras.txt) com pares de palavras corretas-incorretas\n",
        "\n",
        "---\n",
        "\n",
        "* Criando a função **avaliador(teste)** para verificar numericamente qual é a taxa de acerto do corretor (spoiler: é baixa)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JYGoULGv5DQ"
      },
      "source": [
        "### Documentação da aula 04"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMp3l2xTv5DR"
      },
      "source": [
        "* [**](https://)\n",
        "\n",
        "---\n",
        "\n",
        "* [**](https://)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oujm4ONdmdK"
      },
      "source": [
        "## 1) Preparando dados de teste"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6Hq-Q364hzz"
      },
      "source": [
        "Nosso corretor já é capaz de corrigir um único tipo de erro, mas **precisaremos ampliar os operações de correção** para abranger mais casos equivocados.\n",
        "\n",
        "Para comprovarmos se realmente melhoramos a eficiência do algoritmo, compararemos os resultados do corretor inicial com a versão final.\n",
        "\n",
        "Porém, ainda precisamos avaliar o nosso modelo, pois só realizamos testes com a palavra “lógica” até agora."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trnR7dfx42mQ"
      },
      "source": [
        "Definiremos uma função chamada *avaliador()*, que receberá alguns dados de teste, mas ainda não sabemos quais são e os descobriremos adiante. Por enquanto, os chamaremos de *testes*.\n",
        "\n",
        "Com isso, receberemos uma porcentagem da taxa de acerto do meu corretor, e a printaremos com a frase \"taxa de acerto\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NmsrDgY4yRE"
      },
      "source": [
        "## o arquivo palavras.txt possui uma série de palavras escritas de forma incorreta e sua respectiva versão corrigida\n",
        "## esse novo arquivo será utilizado para avaliar o desempenho do corretor\n",
        "\n",
        "## palavra certa /// palavra errada"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7vqHK9a82oU"
      },
      "source": [
        "![print palavras.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFzDFBUF8z2t",
        "outputId": "1bdaac59-ae55-4bf7-f294-e9e1991d770e"
      },
      "source": [
        "## para dar entrada nos dados de teste na função avaliador(testes), devemos criar outra função para criar esses dados de teste com base no arquivo importado\n",
        "\n",
        "def cria_dados_teste(nome_arquivo):\n",
        "  lista_palavras_teste = []                                                     ## lista vazia\n",
        "  f = open(nome_arquivo, \"r\")                                                   ## lendo o arquivo de teste\n",
        "  for linha in f:                                                               ## para cada linha no nosso arquivo, faça o seguinte:\n",
        "    correta, errada = linha.split()                                             ## faça o split de cada linha, dividindo o lado correto do lado errado, de acordo com o print anterior\n",
        "    lista_palavras_teste.append((correta, errada))                              ## adicionando os pares de palavras em formato de tupla (correta,errada) na lista lista_palavras_teste\n",
        "  f.close()                                                                     ## com o arquivo estando aberto, é hora de fechá-lo\n",
        "  return lista_palavras_teste                                                   ## retornando a lista_palavras_teste, que será usada na função avaliador()\n",
        "\n",
        "\n",
        "## vamos dar uma olhada no resultado:\n",
        "\n",
        "lista_teste = cria_dados_teste(\"palavras.txt\")\n",
        "lista_teste"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('podemos', 'pyodemos'),\n",
              " ('esse', 'esje'),\n",
              " ('já', 'jrá'),\n",
              " ('nosso', 'nossov'),\n",
              " ('são', 'sãêo'),\n",
              " ('dos', 'dosa'),\n",
              " ('muito', 'muifo'),\n",
              " ('imagem', 'iômagem'),\n",
              " ('sua', 'ósua'),\n",
              " ('também', 'tambéùm'),\n",
              " ('ele', 'eme'),\n",
              " ('fazer', 'èazer'),\n",
              " ('temos', 'temfs'),\n",
              " ('essa', 'eàssa'),\n",
              " ('quando', 'quaôdo'),\n",
              " ('vamos', 'vamvos'),\n",
              " ('sobre', 'hsobre'),\n",
              " ('java', 'sjava'),\n",
              " ('das', 'daõs'),\n",
              " ('agora', 'agorah'),\n",
              " ('está', 'eòtá'),\n",
              " ('cada', 'céda'),\n",
              " ('mesmo', 'zmesmo'),\n",
              " ('nos', 'noâ'),\n",
              " ('forma', 'fobma'),\n",
              " ('seja', 'sejéa'),\n",
              " ('então', 'enêão'),\n",
              " ('criar', 'èriar'),\n",
              " ('código', 'cóeigo'),\n",
              " ('caso', 'casío'),\n",
              " ('exemplo', 'áexemplo'),\n",
              " ('tem', 'tĩem'),\n",
              " ('usuário', 'usuárôio'),\n",
              " ('dados', 'dfados'),\n",
              " ('python', 'pgthon'),\n",
              " ('nossa', 'nossah'),\n",
              " ('além', 'alémè'),\n",
              " ('assim', 'asõim'),\n",
              " ('ter', 'teb'),\n",
              " ('até', 'atĩ'),\n",
              " ('bem', 'âem'),\n",
              " ('design', 'desigen'),\n",
              " ('trabalho', 'trabalàho'),\n",
              " ('foi', 'foo'),\n",
              " ('apenas', 'apenaũ'),\n",
              " ('empresa', 'empresà'),\n",
              " ('valor', 'valíor'),\n",
              " ('será', 'serr'),\n",
              " ('entre', 'entke'),\n",
              " ('método', 'méqodo'),\n",
              " ('precisamos', 'precisamops'),\n",
              " ('ainda', 'ainàa'),\n",
              " ('vai', 'van'),\n",
              " ('conteúdo', 'ûconteúdo'),\n",
              " ('seus', 'çeus'),\n",
              " ('eu', 'eû'),\n",
              " ('todos', 'todtos'),\n",
              " ('tempo', 'temeo'),\n",
              " ('sempre', 'semre'),\n",
              " ('qual', 'quakl'),\n",
              " ('ela', 'elaá'),\n",
              " ('só', 'síó'),\n",
              " ('utilizar', 'utiqizar'),\n",
              " ('projeto', 'prhojeto'),\n",
              " ('site', 'siàe'),\n",
              " ('sem', 'seém'),\n",
              " ('pelo', 'peln'),\n",
              " ('alura', 'aléra'),\n",
              " ('dia', 'tdia'),\n",
              " ('tudo', 'tuúo'),\n",
              " ('podemos', 'kpodemos'),\n",
              " ('esse', 'eẽsse'),\n",
              " ('já', 'jé'),\n",
              " ('nosso', 'nçosso'),\n",
              " ('são', 'sãô'),\n",
              " ('dos', 'odos'),\n",
              " ('muito', 'tuito'),\n",
              " ('imagem', 'imõgem'),\n",
              " ('sua', 'siua'),\n",
              " ('também', 'tamvbém'),\n",
              " ('ele', 'elpe'),\n",
              " ('fazer', 'façzer'),\n",
              " ('temos', 'teos'),\n",
              " ('essa', 'eũsa'),\n",
              " ('quando', 'quaìdo'),\n",
              " ('vamos', 'vjmos'),\n",
              " ('sobre', 'sxobre'),\n",
              " ('java', 'jkva'),\n",
              " ('das', 'dms'),\n",
              " ('agora', 'agtora'),\n",
              " ('está', 'esútá'),\n",
              " ('cada', 'cava'),\n",
              " ('mesmo', 'medmo'),\n",
              " ('nos', 'ános'),\n",
              " ('forma', 'forûa'),\n",
              " ('seja', 'smeja'),\n",
              " ('então', 'enjtão'),\n",
              " ('criar', 'criôar'),\n",
              " ('código', 'cóàigo'),\n",
              " ('caso', 'èaso'),\n",
              " ('exemplo', 'exbemplo'),\n",
              " ('tem', 'túem'),\n",
              " ('usuário', 'usuárin'),\n",
              " ('dados', 'daáos'),\n",
              " ('python', 'pythoçn'),\n",
              " ('nossa', 'nossk'),\n",
              " ('além', 'âlém'),\n",
              " ('assim', 'aóssim'),\n",
              " ('ter', 'tãer'),\n",
              " ('até', 'vté'),\n",
              " ('bem', 'búm'),\n",
              " ('design', 'íesign'),\n",
              " ('trabalho', 'trabèalho'),\n",
              " ('foi', 'kfoi'),\n",
              " ('apenas', 'aapenas'),\n",
              " ('empresa', 'pmpresa'),\n",
              " ('valor', 'valoqr'),\n",
              " ('será', 'sçerá'),\n",
              " ('entre', 'entró'),\n",
              " ('método', 'nétodo'),\n",
              " ('precisamos', 'prefcisamos'),\n",
              " ('ainda', 'sainda'),\n",
              " ('vai', 'uai'),\n",
              " ('conteúdo', 'cĩonteúdo'),\n",
              " ('seus', 'sâus'),\n",
              " ('eu', 'ìeu'),\n",
              " ('todos', 'todás'),\n",
              " ('tempo', 'utempo'),\n",
              " ('sempre', 'sempce'),\n",
              " ('qual', 'fual'),\n",
              " ('ela', 'elal'),\n",
              " ('só', 'skó'),\n",
              " ('utilizar', 'utilĩzar'),\n",
              " ('projeto', 'proójeto'),\n",
              " ('site', 'isite'),\n",
              " ('sem', 'secm'),\n",
              " ('pelo', 'pẽlo'),\n",
              " ('alura', 'aluéa'),\n",
              " ('dia', 'dil'),\n",
              " ('tudo', 'tudy'),\n",
              " ('ela', 'qelay'),\n",
              " ('só', 'sód'),\n",
              " ('utilizar', 'dtilizacr'),\n",
              " ('projeto', 'bprojõto'),\n",
              " ('site', 'ysiteo'),\n",
              " ('sem', 'sõêm'),\n",
              " ('pelo', 'peàli'),\n",
              " ('alura', 'asuraó'),\n",
              " ('dia', 'deiìa'),\n",
              " ('tudo', 'tuĩdoì'),\n",
              " ('ela', 'eúaa'),\n",
              " ('só', 'ró'),\n",
              " ('utilizar', 'utilizẽaçr'),\n",
              " ('projeto', 'prêjetó'),\n",
              " ('site', 'sqiqte'),\n",
              " ('sem', 'sũexm'),\n",
              " ('pelo', 'pçlxo'),\n",
              " ('alura', 'uluraa'),\n",
              " ('dia', 'dĩaz'),\n",
              " ('tudo', 'kzudo'),\n",
              " ('corretor', 'correptor'),\n",
              " ('tática', 'trtica'),\n",
              " ('empoderamento', 'ewpoderamento'),\n",
              " ('linux', 'lifux'),\n",
              " ('cachorro', 'cachoçro'),\n",
              " ('gato', 'îgato'),\n",
              " ('cavalo', 'cakvalo'),\n",
              " ('relógio', 'relógiuo'),\n",
              " ('canela', 'canelac'),\n",
              " ('tênis', 'tênisy'),\n",
              " ('ansiosa', 'anciosa'),\n",
              " ('ansiosa', 'ancciosa'),\n",
              " ('ansiosa', 'ansioa'),\n",
              " ('empoderamento', 'empoderamento'),\n",
              " ('asterisco', 'asterístico'),\n",
              " ('gratuito', 'gratuíto'),\n",
              " ('entretido', 'entertido'),\n",
              " ('ritmo', 'ritimo'),\n",
              " ('idiota', 'indiota'),\n",
              " ('tomara', 'tomare'),\n",
              " ('seja', 'seje'),\n",
              " ('prevalecer', 'provalecer'),\n",
              " ('esteja', 'esteje'),\n",
              " ('mendigo', 'mindigo'),\n",
              " ('cérebro', 'célebro'),\n",
              " ('perturbar', 'pertubar')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ly4i9hy8dmdL"
      },
      "source": [
        "## 2) Avaliando o corretor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIbPdZyZ4yTk",
        "outputId": "b59016dc-c0c4-4182-c12d-bbe3d712568b"
      },
      "source": [
        "## criando a função avaliador(), usando os dados de teste para saber se o meu corretor é eficiente\n",
        "\n",
        "def avaliador(testes):\n",
        "  n_palavras = len(testes)                                                      ## total de palavras avaliadas, contidas na base de dados de teste\n",
        "  acertos = 0                                                                   ## iniciando um contador; para cada acerto, soma-se 1 ao valor do contador\n",
        "  for correta,errada in testes:                                                 ## para cada tupla contendo um par de palavras (correta,errada) dentro da lista, faça o seguinte:\n",
        "    if corretor(errada) == correta:                                             ## se a palavra errada ficar correta usando o corretor,\n",
        "      acertos += 1                                                              ## some 1 no contador\n",
        "  taxa_acerto = acertos/n_palavras                                              ## cálculo da taxa de acertos = acertos/total de palavras\n",
        "  print(f'Taxa de acerto: {taxa_acerto*100:.2f}% de {n_palavras} palavras')     ## qual foi a taxa de acertos?\n",
        "\n",
        "avaliador(lista_teste)   ## muito baixa\n",
        "\n",
        "## precisamos melhorar o nosso corretor"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Taxa de acerto: 1.08% de 186 palavras\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xf59vzhoeAN6"
      },
      "source": [
        "# <font color=navyblue> `AULA 05` Incrementando o corretor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Smizb4dBwAz7"
      },
      "source": [
        "## Resumo e documentação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtxlPeGCwAz8"
      },
      "source": [
        "### Resumo da aula 05"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuhkEUyBwAz9"
      },
      "source": [
        "* Descrevendo a nova operação para melhorar o corretor: em vez de adicionar uma letra à palavra incorreta, podemos retirar uma letra\n",
        "* O processo funciona da seguinte forma: após os fatiamentos, as partes são concatenadas sem a primeira letra do lado direito\n",
        "\n",
        "---\n",
        "\n",
        "* Criando a função **deletando_caracteres(fatias)** para deletar o primeiro caractere do lado direito do fatiamento e criar novas palavras\n",
        "\n",
        "---\n",
        "\n",
        "* Adicionando a função **deletando_caracteres(fatias)** na cadeia de funções do corretor e testando sua precisão"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLxkcRMuwAz9"
      },
      "source": [
        "### Documentação da aula 05"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nKhl2nCkKNo"
      },
      "source": [
        "* [**](https://)\n",
        "\n",
        "---\n",
        "\n",
        "* [**](https://)\n",
        "\n",
        "---\n",
        "\n",
        "* [**](https://)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBvUD0FpeAN7"
      },
      "source": [
        "## 1) Operação de delete"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00BHs5Y4ko_l"
      },
      "source": [
        "A operação que realizamos para corrigir o primeiro problema da falta de um caractere em \"lgica\" insere uma letra em todas as posições possíveis da palavra, gerando vários resultados.\n",
        "\n",
        "Neste passo, corrigiremos um outro tipo de erro bastante comum também além de deletar ou esquecer de digitar, que consiste em escrever *uma letra a mais* por engano."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph7rFobnlDUy"
      },
      "source": [
        "Como corrigir palavras que possuem uma letra a mais? Usemos como exemplo a palavra *lóigica* -- \"lógica\" escrita com uma letra a mais\n",
        "\n",
        "Sobre a operação: dividiremos as palavras em duas partes -- o lado esquerdo e o lado direito. O lado esquerdo será totalmente preservado; já o lado direito terá sempre a primeira letra subtraída.\n",
        "\n",
        "*esquerdo - LETRA + direito = palavra*\n",
        "\n",
        "Mas em qual parte da palavra haverá essa divisão?\n",
        "* null + lóigica - l  ------->  óigica\n",
        "* l + óigica - ó  ------------>  ligica\n",
        "* ló + igica - i  ------------->  lógica\n",
        "* lói + gica - g  ------------>  lóiica\n",
        "* lóig + ica - i  ------------->  lóigca\n",
        "* lóigi + ca - c  ------------>  lóigia\n",
        "* lóigic + a - a  ------------>  lóigia\n",
        "* lóigica + null - null -->  lóigica\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_BoTJL_eAN7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "58291a12-e389-4233-d5fa-2eb3a0c59598"
      },
      "source": [
        "sss = \"lóigica\"\n",
        "sss[:2] + sss[2:][1:]\n",
        "\n",
        "## lado esquerdo + lado direito [1:]\n",
        "  ## o lado esquerdo é preservado; do lado direito é retirada a primeira letra\n",
        "  ## após a separação, basta concatenar"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'lógica'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5I3rD8-eAN8"
      },
      "source": [
        "## 2) Implementando o delete de caracteres"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ncvkx1yeAN7"
      },
      "source": [
        "## considerando que a lista de tuplas com as fatias já está pronta, a função deletando_caracteres() será usada apenas para gerar uma nova lista de possíveis palavras ao retirar um caractere \n",
        "\n",
        "def deletando_caracteres(fatias):\n",
        "  novas_palavras = []\n",
        "  for E,D in fatias:                                              ## para cada lado esquerdo e direito das minhas palavras\n",
        "    novas_palavras.append(E + D[1:])                              ## esquerdo + direito[1:] = palavra nova\n",
        "  return novas_palavras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd4ji2SneAOB"
      },
      "source": [
        "## 3) Avaliando o novo corretor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-E0yMcQeAN7"
      },
      "source": [
        "## modifiquemos a função gerador_palavras(), acrescentando o novo método à lista de palavras geradas\n",
        "\n",
        "def gerador_palavras(palavra):                        ## criando a função\n",
        "  fatias = []\n",
        "  for i in range(len(palavra)+1):                     ## criando um for para fatiar as palavras em duas partes; lembrar de somar +1 pra pegar todos os fatiamentos\n",
        "    fatias.append((palavra[:i],palavra[i:]))          ## cada fatiamento vira uma tupla com as duas partes da palavra dividida -- dando append das tuplas na lista fatias\n",
        "  palavras_geradas = insere_letras(fatias)            ## as palavras geradas virão de outra função, que inserirá letras nas fatias da esquerda e da direita da palavra\n",
        "  palavras_geradas += deletando_caracteres(fatias)    ## ADICIONANDO NOVAS PALAVRAS AO CORRETOR -- para os casos que a palavra incorreta possui uma letra a mais\n",
        "  return palavras_geradas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJCQZoBReAN7"
      },
      "source": [
        "## recapitulando:\n",
        "  ## avaliador(lista_teste) --------------------> calcula a precisão do corretor\n",
        "    ## corretor(palavra) -----------------------> calcula a probabilidade de cada elemento da lista de palavras geradas e retorna qual delas é a palavra corrigida\n",
        "      ## gerador_palavras(palavra) -------------> fatia palavras em E e D e faz a correção usando as funções abaixo\n",
        "        ## insere_letra(fatias) ----------------> insere letras após o fatiamento e gera novas palavras\n",
        "        ## deletando_caracteres(fatia) [NOVO] --> retira a primeira letra de D após o fatiamento e gera novas palavras\n",
        "\n",
        "## uma função dentro da outra"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8YYxBVmeAN8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae87b59d-a0bb-45c6-c53b-14718ba8a9bd"
      },
      "source": [
        "## como eu alterei a cadeia de funções, testemos novamente o corretor\n",
        "## usemos a função avaliador(lista_teste) para isso\n",
        "\n",
        "avaliador(lista_teste)  ## já aumentou a taxa de acerto!!"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Taxa de acerto: 41.40% de 186 palavras\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Auw0_f-FecCA"
      },
      "source": [
        "# <font color=navyblue> `AULA 06` Corrigindo os principais erros de digitação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hih5wTcowFfP"
      },
      "source": [
        "## Resumo e documentação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCIYoIAFwFfQ"
      },
      "source": [
        "### Resumo da aula 06"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oE89Hi1RwFfR"
      },
      "source": [
        "* Próximo passo: fazer com que o corretor corrija palavras trocando letras (tirando uma letra e inserindo outra)\n",
        "\n",
        "---\n",
        "\n",
        "* Criando a função **troca_letras(fatias)** para retirar uma letra e inserir outra\n",
        "* Corrigindo a função **gerador_palavras(palavra)** e inserindo a nova função **troca_letras(fatias)**\n",
        "\n",
        "---\n",
        "\n",
        "* Próximo passo: fazer com que o corretor corrija palavras invertendo o lugar das letras\n",
        "\n",
        "---\n",
        "\n",
        "* Criando a função **insere_letras(fatias)** para inverter o lugar das letras vizinhas\n",
        "* Corrigindo a função **gerador_palavras(palavra)** e inserindo a nova função **insere_letras(fatias)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgCSMzYzwFfR"
      },
      "source": [
        "### Documentação da aula 06\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXy8Zpxf331x"
      },
      "source": [
        "* [**](https://)\n",
        "\n",
        "---\n",
        "\n",
        "* [**](https://)\n",
        "\n",
        "---\n",
        "\n",
        "* [**](https://)\n",
        "\n",
        "---\n",
        "\n",
        "* [**](https://)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maoe4cgnecCD"
      },
      "source": [
        "## 1) Trocando as letras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9IGvSlqlJr6"
      },
      "source": [
        "Nosso corretor já é capaz de resolver dois tipos de erros de digitação.\n",
        "\n",
        "As operações consistem em dividir a string equivocada em duas partes e adicionar ou eliminar um caractere para obter diferentes resultados. Para chegar na palavra correta,foram criadas as funções insere_letras() e deletando_caracteres(), respectivamente.\n",
        "\n",
        "Porém, ainda existem outros tipos de erros, como quando *trocamos uma letra por outra*. Por exemplo, na hora de digitar o “ó” no teclado, sem querer apertamos a tecla “í” que está bem ao lado do caractere correto, gerando a palavra errada \"lígica\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6AHaDnnlJpG"
      },
      "source": [
        "Como corrigir palavras que possuem uma letra errada? Usemos como exemplo a palavra *lígica* -- \"lógica\", trocando \"ó\" por \"í\".\n",
        "\n",
        "Sobre a operação:\n",
        "\n",
        "1) dividiremos as palavras em duas partes -- o lado esquerdo e o lado direito. O lado esquerdo será totalmente preservado; já o lado direito terá sempre a primeira letra subtraída.\n",
        "\n",
        "2) adicionaremos uma letra ao final do lado esquerdo e concatenaremos o resultado.\n",
        "\n",
        "*(esquerdo + LETRA¹) + (direito - LETRA²) = palavra*\n",
        "\n",
        "Mas em qual parte da palavra haverá essa divisão?\n",
        "* (null + LETRA¹) + (lígica - l)  ----->  óigica\n",
        "* (l + LETRA¹) + (ígica - í)  ----------->  **lógica**\n",
        "* (lí + LETRA¹) + (gica - g)  ---------->  lígóca\n",
        "* (líg + LETRA¹) + (ica - i)  ----------->  lígóca\n",
        "* (lígi + LETRA¹) + (ca - c)  ---------->  lígióa\n",
        "\n",
        "* (lígic + LETRA¹) + (a - a)  ---------->  lígicó\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMB_mwtAo-VT"
      },
      "source": [
        "Na prática, é como se fosse a soma das duas operações anteriores:\n",
        "\n",
        "***E + letra + D[:1]***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35Okn2jHecCE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d6ad01b5-6a36-469d-8491-d49c05703965"
      },
      "source": [
        "p = 'lígica'\n",
        "p[:1] + 'ó' + p[1:][1:]  ## esquerda + letra + direita[1:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'lógica'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQopSmpBecCE"
      },
      "source": [
        "## 2) Implementando a troca de letras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_2-ucVUecCD"
      },
      "source": [
        "## estas são as funções que usaremos como base\n",
        "\n",
        "\n",
        "def insere_letras(fatias):\n",
        "  novas_palavras = []\n",
        "  letras = 'abcdefghijklmnopqrstuvwxyzàáâãèéêìíîòóôõùúûç'         ## todas as letras\n",
        "  for E,D in fatias:                                              ## para cada lado esquerdo e direito das minhas palavras\n",
        "    for letra in letras:                                          ## para cada letra a ser inserida\n",
        "      novas_palavras.append(E + letra + D)                        ## esquerdo + letra + direito = palavra nova\n",
        "  return novas_palavras\n",
        "\n",
        "\n",
        "def deletando_caracteres(fatias):\n",
        "  novas_palavras = []\n",
        "  for E,D in fatias:                                              ## para cada lado esquerdo e direito das minhas palavras\n",
        "    novas_palavras.append(E + D[1:])                              ## esquerdo + direito[1:] = palavra nova\n",
        "  return novas_palavras\n",
        "\n",
        "\n",
        "## criando a nova função troca_letras()\n",
        "def troca_letras(fatias):\n",
        "  novas_palavras = []\n",
        "  letras = 'abcdefghijklmnopqrstuvwxyzàáâãèéêìíîòóôõùúûç'         ## todas as letras\n",
        "  for E,D in fatias:                                              ## para cada lado esquerdo e direito das minhas palavras\n",
        "    for letra in letras:                                          ## para cada letra a ser inserida\n",
        "      novas_palavras.append(E + letra + D[1:])                    ## esquerdo + letra + direito[1:] = palavra nova\n",
        "  return novas_palavras\n",
        "\n",
        "\n",
        "## modifiquemos a função gerador_palavras(), acrescentando o novo método à lista de palavras geradas\n",
        "def gerador_palavras(palavra):\n",
        "  fatias = []\n",
        "  for i in range(len(palavra)+1):                     ## criando um for para fatiar as palavras em duas partes; lembrar de somar +1 pra pegar todos os fatiamentos\n",
        "    fatias.append((palavra[:i],palavra[i:]))          ## cada fatiamento vira uma tupla com as duas partes da palavra dividida -- dando append das tuplas na lista fatias\n",
        "  palavras_geradas = insere_letras(fatias)            ## as palavras geradas virão de outra função, que inserirá letras nas fatias da esquerda e da direita da palavra\n",
        "  palavras_geradas += deletando_caracteres(fatias)    ## ADICIONANDO NOVAS PALAVRAS AO CORRETOR -- para os casos que a palavra incorreta possui uma letra a mais\n",
        "  palavras_geradas += troca_letras(fatias)            ## ADICIONANDO NOVAS PALAVRAS AO CORRETOR -- para os casos que a palavra incorreta possui uma letra errada, necessitando ser trocada\n",
        "  return palavras_geradas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1oGx3KbecCE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "741b18bf-fac7-4e4e-aa6a-42867aa3ce75"
      },
      "source": [
        "print(gerador_palavras('lígica'))\n",
        "\n",
        "## gerando palavras de três formas: inserindo letras, retirando letras e trocando letras"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['alígica', 'blígica', 'clígica', 'dlígica', 'elígica', 'flígica', 'glígica', 'hlígica', 'ilígica', 'jlígica', 'klígica', 'llígica', 'mlígica', 'nlígica', 'olígica', 'plígica', 'qlígica', 'rlígica', 'slígica', 'tlígica', 'ulígica', 'vlígica', 'wlígica', 'xlígica', 'ylígica', 'zlígica', 'àlígica', 'álígica', 'âlígica', 'ãlígica', 'èlígica', 'élígica', 'êlígica', 'ìlígica', 'ílígica', 'îlígica', 'òlígica', 'ólígica', 'ôlígica', 'õlígica', 'ùlígica', 'úlígica', 'ûlígica', 'çlígica', 'laígica', 'lbígica', 'lcígica', 'ldígica', 'leígica', 'lfígica', 'lgígica', 'lhígica', 'liígica', 'ljígica', 'lkígica', 'llígica', 'lmígica', 'lnígica', 'loígica', 'lpígica', 'lqígica', 'lrígica', 'lsígica', 'ltígica', 'luígica', 'lvígica', 'lwígica', 'lxígica', 'lyígica', 'lzígica', 'làígica', 'láígica', 'lâígica', 'lãígica', 'lèígica', 'léígica', 'lêígica', 'lìígica', 'líígica', 'lîígica', 'lòígica', 'lóígica', 'lôígica', 'lõígica', 'lùígica', 'lúígica', 'lûígica', 'lçígica', 'líagica', 'líbgica', 'lícgica', 'lídgica', 'líegica', 'lífgica', 'líggica', 'líhgica', 'líigica', 'líjgica', 'líkgica', 'lílgica', 'límgica', 'língica', 'líogica', 'lípgica', 'líqgica', 'lírgica', 'lísgica', 'lítgica', 'líugica', 'lívgica', 'líwgica', 'líxgica', 'líygica', 'lízgica', 'líàgica', 'líágica', 'líâgica', 'líãgica', 'líègica', 'líégica', 'líêgica', 'líìgica', 'líígica', 'líîgica', 'líògica', 'líógica', 'líôgica', 'líõgica', 'líùgica', 'líúgica', 'líûgica', 'líçgica', 'lígaica', 'lígbica', 'lígcica', 'lígdica', 'lígeica', 'lígfica', 'líggica', 'líghica', 'lígiica', 'lígjica', 'lígkica', 'líglica', 'lígmica', 'lígnica', 'lígoica', 'lígpica', 'lígqica', 'lígrica', 'lígsica', 'lígtica', 'líguica', 'lígvica', 'lígwica', 'lígxica', 'lígyica', 'lígzica', 'lígàica', 'lígáica', 'lígâica', 'lígãica', 'lígèica', 'lígéica', 'lígêica', 'lígìica', 'lígíica', 'lígîica', 'lígòica', 'lígóica', 'lígôica', 'lígõica', 'lígùica', 'lígúica', 'lígûica', 'lígçica', 'lígiaca', 'lígibca', 'lígicca', 'lígidca', 'lígieca', 'lígifca', 'lígigca', 'lígihca', 'lígiica', 'lígijca', 'lígikca', 'lígilca', 'lígimca', 'líginca', 'lígioca', 'lígipca', 'lígiqca', 'lígirca', 'lígisca', 'lígitca', 'lígiuca', 'lígivca', 'lígiwca', 'lígixca', 'lígiyca', 'lígizca', 'lígiàca', 'lígiáca', 'lígiâca', 'lígiãca', 'lígièca', 'lígiéca', 'lígiêca', 'lígiìca', 'lígiíca', 'lígiîca', 'lígiòca', 'lígióca', 'lígiôca', 'lígiõca', 'lígiùca', 'lígiúca', 'lígiûca', 'lígiçca', 'lígicaa', 'lígicba', 'lígicca', 'lígicda', 'lígicea', 'lígicfa', 'lígicga', 'lígicha', 'lígicia', 'lígicja', 'lígicka', 'lígicla', 'lígicma', 'lígicna', 'lígicoa', 'lígicpa', 'lígicqa', 'lígicra', 'lígicsa', 'lígicta', 'lígicua', 'lígicva', 'lígicwa', 'lígicxa', 'lígicya', 'lígicza', 'lígicàa', 'lígicáa', 'lígicâa', 'lígicãa', 'lígicèa', 'lígicéa', 'lígicêa', 'lígicìa', 'lígicía', 'lígicîa', 'lígicòa', 'lígicóa', 'lígicôa', 'lígicõa', 'lígicùa', 'lígicúa', 'lígicûa', 'lígicça', 'lígicaa', 'lígicab', 'lígicac', 'lígicad', 'lígicae', 'lígicaf', 'lígicag', 'lígicah', 'lígicai', 'lígicaj', 'lígicak', 'lígical', 'lígicam', 'lígican', 'lígicao', 'lígicap', 'lígicaq', 'lígicar', 'lígicas', 'lígicat', 'lígicau', 'lígicav', 'lígicaw', 'lígicax', 'lígicay', 'lígicaz', 'lígicaà', 'lígicaá', 'lígicaâ', 'lígicaã', 'lígicaè', 'lígicaé', 'lígicaê', 'lígicaì', 'lígicaí', 'lígicaî', 'lígicaò', 'lígicaó', 'lígicaô', 'lígicaõ', 'lígicaù', 'lígicaú', 'lígicaû', 'lígicaç', 'ígica', 'lgica', 'líica', 'lígca', 'lígia', 'lígic', 'lígica', 'aígica', 'bígica', 'cígica', 'dígica', 'eígica', 'fígica', 'gígica', 'hígica', 'iígica', 'jígica', 'kígica', 'lígica', 'mígica', 'nígica', 'oígica', 'pígica', 'qígica', 'rígica', 'sígica', 'tígica', 'uígica', 'vígica', 'wígica', 'xígica', 'yígica', 'zígica', 'àígica', 'áígica', 'âígica', 'ãígica', 'èígica', 'éígica', 'êígica', 'ìígica', 'íígica', 'îígica', 'òígica', 'óígica', 'ôígica', 'õígica', 'ùígica', 'úígica', 'ûígica', 'çígica', 'lagica', 'lbgica', 'lcgica', 'ldgica', 'legica', 'lfgica', 'lggica', 'lhgica', 'ligica', 'ljgica', 'lkgica', 'llgica', 'lmgica', 'lngica', 'logica', 'lpgica', 'lqgica', 'lrgica', 'lsgica', 'ltgica', 'lugica', 'lvgica', 'lwgica', 'lxgica', 'lygica', 'lzgica', 'làgica', 'lágica', 'lâgica', 'lãgica', 'lègica', 'légica', 'lêgica', 'lìgica', 'lígica', 'lîgica', 'lògica', 'lógica', 'lôgica', 'lõgica', 'lùgica', 'lúgica', 'lûgica', 'lçgica', 'líaica', 'líbica', 'lícica', 'lídica', 'líeica', 'lífica', 'lígica', 'líhica', 'líiica', 'líjica', 'líkica', 'lílica', 'límica', 'línica', 'líoica', 'lípica', 'líqica', 'lírica', 'lísica', 'lítica', 'líuica', 'lívica', 'líwica', 'líxica', 'líyica', 'lízica', 'líàica', 'líáica', 'líâica', 'líãica', 'líèica', 'líéica', 'líêica', 'líìica', 'lííica', 'líîica', 'líòica', 'líóica', 'líôica', 'líõica', 'líùica', 'líúica', 'líûica', 'líçica', 'lígaca', 'lígbca', 'lígcca', 'lígdca', 'lígeca', 'lígfca', 'líggca', 'líghca', 'lígica', 'lígjca', 'lígkca', 'líglca', 'lígmca', 'lígnca', 'lígoca', 'lígpca', 'lígqca', 'lígrca', 'lígsca', 'lígtca', 'líguca', 'lígvca', 'lígwca', 'lígxca', 'lígyca', 'lígzca', 'lígàca', 'lígáca', 'lígâca', 'lígãca', 'lígèca', 'lígéca', 'lígêca', 'lígìca', 'lígíca', 'lígîca', 'lígòca', 'lígóca', 'lígôca', 'lígõca', 'lígùca', 'lígúca', 'lígûca', 'lígçca', 'lígiaa', 'lígiba', 'lígica', 'lígida', 'lígiea', 'lígifa', 'lígiga', 'lígiha', 'lígiia', 'lígija', 'lígika', 'lígila', 'lígima', 'lígina', 'lígioa', 'lígipa', 'lígiqa', 'lígira', 'lígisa', 'lígita', 'lígiua', 'lígiva', 'lígiwa', 'lígixa', 'lígiya', 'lígiza', 'lígiàa', 'lígiáa', 'lígiâa', 'lígiãa', 'lígièa', 'lígiéa', 'lígiêa', 'lígiìa', 'lígiía', 'lígiîa', 'lígiòa', 'lígióa', 'lígiôa', 'lígiõa', 'lígiùa', 'lígiúa', 'lígiûa', 'lígiça', 'lígica', 'lígicb', 'lígicc', 'lígicd', 'lígice', 'lígicf', 'lígicg', 'lígich', 'lígici', 'lígicj', 'lígick', 'lígicl', 'lígicm', 'lígicn', 'lígico', 'lígicp', 'lígicq', 'lígicr', 'lígics', 'lígict', 'lígicu', 'lígicv', 'lígicw', 'lígicx', 'lígicy', 'lígicz', 'lígicà', 'lígicá', 'lígicâ', 'lígicã', 'lígicè', 'lígicé', 'lígicê', 'lígicì', 'lígicí', 'lígicî', 'lígicò', 'lígicó', 'lígicô', 'lígicõ', 'lígicù', 'lígicú', 'lígicû', 'lígicç', 'lígicaa', 'lígicab', 'lígicac', 'lígicad', 'lígicae', 'lígicaf', 'lígicag', 'lígicah', 'lígicai', 'lígicaj', 'lígicak', 'lígical', 'lígicam', 'lígican', 'lígicao', 'lígicap', 'lígicaq', 'lígicar', 'lígicas', 'lígicat', 'lígicau', 'lígicav', 'lígicaw', 'lígicax', 'lígicay', 'lígicaz', 'lígicaà', 'lígicaá', 'lígicaâ', 'lígicaã', 'lígicaè', 'lígicaé', 'lígicaê', 'lígicaì', 'lígicaí', 'lígicaî', 'lígicaò', 'lígicaó', 'lígicaô', 'lígicaõ', 'lígicaù', 'lígicaú', 'lígicaû', 'lígicaç']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yTzoCLlecCE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "db2bb9bd-95ff-4648-ed49-e548835683a7"
      },
      "source": [
        "corretor('lígica')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'lógica'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlTN3hh1GZ9b"
      },
      "source": [
        "## 3) Invertendo as letras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFgqp-ZOwenl"
      },
      "source": [
        "Neste passo, corrigiremos um erro de digitação onde letras foram invertidas em suas posições, como na palavra escrita errada \"lgóica\".\n",
        "\n",
        "Neste caso, a posição da letra “g” foi trocada com a letra “ó”; como se trata de um equívoco bastante comum, precisaremos deixar o corretor cada vez mais abrangente. Portanto, implementaremos um algoritmo capaz de realizar esse tipo de correção.\n",
        "\n",
        "***E + D[1] + D[0] + D[2:]***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9ctNWExGZ9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2927b093-723d-4739-8c8b-b8b20c7d772c"
      },
      "source": [
        "p = 'lgóica'\n",
        "esqp = p[:4]\n",
        "dirp = p[4:]\n",
        "\n",
        "esqp + dirp[1] + dirp[0] + dirp[2:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'lgóiac'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCN5v153GZ9m"
      },
      "source": [
        "## 4) Codando a inversão de letras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEWCY6AqGZ9h"
      },
      "source": [
        "def inverte_letra(fatias):\n",
        "  novas_palavras = []\n",
        "  for E,D in fatias:                                                ## para cada lado esquerdo e direito das minhas palavras\n",
        "    if len(D) >= 2:                                                 ## se a fatia do lado direito tiver pelo menos duas letras, faça o seguinte:\n",
        "      novas_palavras.append(E + D[1] + D[0] + D[2:])                ## esquerdo + direito[1] + direito[0] + direito[1:] = palavra nova\n",
        "  return novas_palavras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBmsQiXFGZ9k"
      },
      "source": [
        "## corrigindo com todas as funções\n",
        "\n",
        "def insere_letras(fatias):\n",
        "  novas_palavras = []\n",
        "  letras = 'abcdefghijklmnopqrstuvwxyzàáâãèéêìíîòóôõùúûç'         ## todas as letras\n",
        "  for E,D in fatias:                                              ## para cada lado esquerdo e direito das minhas palavras\n",
        "    for letra in letras:                                          ## para cada letra a ser inserida\n",
        "      novas_palavras.append(E + letra + D)                        ## esquerdo + letra + direito = palavra nova\n",
        "  return novas_palavras\n",
        "\n",
        "\n",
        "def deletando_caracteres(fatias):\n",
        "  novas_palavras = []\n",
        "  for E,D in fatias:                                              ## para cada lado esquerdo e direito das minhas palavras\n",
        "    novas_palavras.append(E + D[1:])                              ## esquerdo + direito[1:] = palavra nova\n",
        "  return novas_palavras\n",
        "\n",
        "\n",
        "def troca_letras(fatias):\n",
        "  novas_palavras = []\n",
        "  letras = 'abcdefghijklmnopqrstuvwxyzàáâãèéêìíîòóôõùúûç'         ## todas as letras\n",
        "  for E,D in fatias:                                              ## para cada lado esquerdo e direito das minhas palavras\n",
        "    for letra in letras:                                          ## para cada letra a ser inserida\n",
        "      novas_palavras.append(E + letra + D[1:])                    ## esquerdo + letra + direito[1:] = palavra nova\n",
        "  return novas_palavras\n",
        "\n",
        "\n",
        "## criando a nova função inverte_letras()\n",
        "def inverte_letras(fatias):\n",
        "  novas_palavras = []\n",
        "  for E,D in fatias:                                                ## para cada lado esquerdo e direito das minhas palavras\n",
        "    if len(D) > 1:                                                 ## se a fatia do lado direito tiver pelo menos duas letras, faça o seguinte:\n",
        "      novas_palavras.append(E + D[1] + D[0] + D[2:])                ## esquerdo + direito[1] + direito[0] + direito[1:] = palavra nova\n",
        "  return novas_palavras\n",
        "\n",
        "\n",
        "## modifiquemos a função gerador_palavras(), acrescentando o novo método à lista de palavras geradas\n",
        "def gerador_palavras(palavra):\n",
        "  fatias = []\n",
        "  for i in range(len(palavra)+1):                     ## criando um for para fatiar as palavras em duas partes; lembrar de somar +1 pra pegar todos os fatiamentos\n",
        "    fatias.append((palavra[:i],palavra[i:]))          ## cada fatiamento vira uma tupla com as duas partes da palavra dividida -- dando append das tuplas na lista fatias\n",
        "  palavras_geradas = insere_letras(fatias)            ## as palavras geradas virão de outra função, que inserirá letras nas fatias da esquerda e da direita da palavra\n",
        "  palavras_geradas += deletando_caracteres(fatias)    ## ADICIONANDO NOVAS PALAVRAS AO CORRETOR -- para os casos que a palavra incorreta possui uma letra a mais\n",
        "  palavras_geradas += troca_letras(fatias)            ## ADICIONANDO NOVAS PALAVRAS AO CORRETOR -- para os casos que a palavra incorreta possui uma letra errada, necessitando ser trocada por outra\n",
        "  palavras_geradas += inverte_letras(fatias)          ## ADICIONANDO NOVAS PALAVRAS AO CORRETOR -- para os casos que a palavra incorreta possui um erro no posicionamento das letras vizinhas\n",
        "  return palavras_geradas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xA7h0vQAGZ9l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e20db8c9-958c-47b4-8764-4c81882ac464"
      },
      "source": [
        "len(gerador_palavras('lgóica'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "628"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgEl8QJnGZ9n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "56490c73-1a84-49da-bc5d-bbb815758d59"
      },
      "source": [
        "corretor('lógiac')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'lógica'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3qogJi_GZ9o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "21b89369-bbbc-40bc-aaa0-d40fbc3ddad3"
      },
      "source": [
        "avaliador(lista_teste)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-a3d90c8c9c7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mavaliador\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlista_teste\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: avaliador() missing 1 required positional argument: 'vocabulario'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8n_j2DWWB770"
      },
      "source": [
        "# <font color=navyblue> `AULA 07` Criando um corretor turbinado\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dE5_kKUrwJ-P"
      },
      "source": [
        "## Resumo e documentação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O13y-AJxwJ-Q"
      },
      "source": [
        "### Resumo da aula 07"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIWmJjJC55z-"
      },
      "source": [
        "* Qual o percentual de erros por causa da limitação do nosso vocabulário? Adaptamos a função **avaliador(testes, vocabulario)** para conseguir essa respostas\n",
        "\n",
        "---\n",
        "\n",
        "* Construção da função **gerador_turbinado(palavras_geradas)**, permitindo que cada palavra passe duas vezes pela cadeia de funções do corretor\n",
        "\n",
        "---\n",
        "\n",
        "* Como geramos muitas palavras após a criação do **gerador_turbinado(palavras_geradas)**, precisamos reduzir essa lista para contemplar apenas aquelas que são mais próváveis de serem a versão corrigida da palavra inicial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUO-PmsrwJ-R"
      },
      "source": [
        "### Documentação da aula 07"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPIWxOT8wJ-S"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wLeZmVPB772"
      },
      "source": [
        "## 1) Palavras desconhecidas ao vocabulário"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1A_P7AX52GW"
      },
      "source": [
        "Nosso corretor já é capaz de acertar aproximadamente 77% das palavras.\n",
        "\n",
        "A taxa de erros está associada a dois fatore: restrição do algoritmo que não gera a palavra correta e o tamanho dos dados de treinamento.\n",
        "\n",
        "Às vezes, um determinado termo ainda é desconhecido e não podemos realizar a correção de algo que não conhecemos.\n",
        "\n",
        "O modelo do corretor é restrito por este volume de vocabulário, calculado em 17.654 vocábulos.\n",
        "\n",
        "Precisaremos calcular o quanto essa restrição está impactando no resultado, pois temos uma taxa de erro e deveremos saber o quanto desta porcentagem está associada às palavras desconhecidas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ci7Olgs6xDi",
        "outputId": "3b2d3073-b5b5-4cf0-b3a0-72d58b708704"
      },
      "source": [
        "## ajustando a função avaliador() para descobrir qual o peso do desconhecimento das palavras no erro do corretor\n",
        "\n",
        "\n",
        "def avaliador(testes, vocabulario):\n",
        "  n_palavras = len(testes)                                                      ## total de palavras avaliadas, contidas na base de dados de teste\n",
        "  acertos = 0                                                                   ## contador para palavras que o corretor acertou\n",
        "  desconhecida = 0                                                              ## contador para palavras desconhecidas\n",
        "  for correta,errada in testes:                                                 ## para cada tupla contendo um par de palavras (correta,errada) dentro da lista, faça o seguinte:\n",
        "    if corretor(errada) == correta:                                             ## se a palavra errada ficar correta usando o corretor,\n",
        "      acertos += 1                                                              ## some 1 no contador\n",
        "    elif correta not in vocabulario:                                            ## considerando que a palavra não foi corrigida: se a versão correta da palavra não estiver no vocabulário\n",
        "      desconhecida += 1                                                         ## adicione 1 ao contador desconhecida\n",
        "  taxa_acerto = acertos/n_palavras                                                             ## cálculo da taxa de acertos = acertos/total de palavras\n",
        "  taxa_desconhecida = desconhecida/n_palavras                                                  ## cálculo da taxa de desconhecimento = desconehcida/total de palavras\n",
        "  print(f'Taxa de acerto: {taxa_acerto*100:.2f}% de {n_palavras} palavras')                    ## qual foi a taxa de acertos?\n",
        "  print(f'Taxa de desconhecimento: {taxa_desconhecida*100:.2f}% de {n_palavras} palavras')     ## qual foi a taxa de desconhecimento das palavras?\n",
        "\n",
        "vocabulario = set(lista_normalizada)                                            ## todas as palavras do nosso vocabulário\n",
        "\n",
        "avaliador(lista_teste, vocabulario)                                             ## avaliador com dois parâmetros"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Taxa de acerto: 76.34% de 186 palavras\n",
            "Taxa de desconhecimento: 6.99% de 186 palavras\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqpiLubwB776"
      },
      "source": [
        "## 2) Turbinando o gerador de palavras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Av-AndznFuab"
      },
      "source": [
        "Quando a função gerador_palavras(palavra) entra em ação, a palavra errada que foi informada como dado de entrada será fatiada em duas partes e passará por quatro processos antes de ser concatenada: inserção, retirada, troca e inversão de letras. Essa lista de processos gerará uma grande quantidade de palavras e espera-se que uma delas seja a versão correta da palavra inserida na função.\n",
        "\n",
        "E se essa lista de palavras passasse *novamente* por essa cadeia de funções? Se uma palavra gera centenas de outras ao passar pelas funções, imagine quantas seriam formadas se *todas as palavras geradas* passassem pelo mesmo processo?\n",
        "\n",
        "No caso de *lóiigica*, por exemplo, duas letras precisariam ser removidas para encontrarmos a palavra correta. Ou seja, a palavra teria que passar mais de uma vez pela cadeia de funções.\n",
        "\n",
        "lóiigica -- cadeia de funções --> lóigica -- cadeia de funções --> lógica\n",
        "\n",
        "\n",
        "Faremos o seguinte: vamos construir a função gerador_turbinado(palavras_geradas), que terá como dado de entrada a lista de palavras geradas na primeira passagem pela cadeia de funções. Todas elas gerarão uma outra lista de palavras com possíveis correções da palavra inicial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgQ0Az1iB774",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "188d1f36-1fcc-441f-ecae-74700d39be1a"
      },
      "source": [
        "palavra = 'lóiigica'\n",
        "\n",
        "def gerador_turbinado(palavras_geradas):\n",
        "  novas_palavras = []                                       ## novas_palavras -> lista vazia\n",
        "  for palavra in palavras_geradas:                          ## para cada palavra da lista de palavras geradas na primeira passagem pela função:\n",
        "    novas_palavras += gerador_palavras(palavra)             ## adicione a lista de palavras geradas na lista novas_palavras\n",
        "  return novas_palavras                                     ## nova lista de palavras\n",
        "\n",
        "palavra in gerador_turbinado(gerador_palavras(palavra))     ## será que ele corrige a palavra 'lóiigica'?"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDWYQWRsB774",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2af18dae-3577-4dcc-c459-c2bafafafaf0"
      },
      "source": [
        "len(gerador_turbinado(gerador_palavras('lóiigica')))\n",
        "\n",
        "## saiu de 600 e poucas palavras pra quase 700 mil"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "691744"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IH4HnjS9GwPc"
      },
      "source": [
        "## 3) Escolhendo os melhores candidatos\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eihhVPXNVhkg"
      },
      "source": [
        "Anteriormente, vimos um gigantesco número de palavras geradas para uma única correção, tanto pelo **gerador_palavras(**) quanto pelo **gerador_turbinado()**.\n",
        "\n",
        "Por conta disso, calcular a máxima probabilidade de todas esses possíveis resultados corretos se tornou inviável. Assim, precisaremos reduzir este número de candidatos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "l-GVzwbnVhSE",
        "outputId": "5bf4285e-e141-47b0-ea4e-882d95266827"
      },
      "source": [
        "def novo_corretor(palavra):\n",
        "    palavras_geradas = gerador_palavras(palavra)                                ## palavras geradas pelo corretor inicial\n",
        "    palavras_turbinado = gerador_turbinado(palavras_geradas)                    ## palavras geradas pelo corretor turbinado\n",
        "    todas_palavras = set(palavras_geradas + palavras_turbinado)                 ## a junção das palavras geradas pelos corretores\n",
        "    \n",
        "    candidatas = [palavra]                                                      ## lista contendo possíveis palavras corretas\n",
        "    for palavra1 in todas_palavras:                                             ## para cada palavra contida na lista todas_palavras, que contém todas as palavras dos dois corretores:\n",
        "      if palavra1 in vocabulario:                                               ## se essa palavra estiver no vocabulário (a chance dela ser a palavra correta é grande):\n",
        "        candidatas.append(palavra1)                                             ## coloque-a na lista de candidatas, já que é bem provável que uma dela seja a palavra correta\n",
        "\n",
        "    palavra_correta = max(candidatas, key=probabilidade)                        ## calculemos a probabilidade dentre as palavras da lista candidatas e vejamos qual delas possui o maior valor\n",
        "    return palavra_correta\n",
        "\n",
        "novo_corretor(\"lóiigica\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'lógica'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vq5eqOrUCWMx"
      },
      "source": [
        "# <font color=navyblue> `AULA 08` Avaliando e interpretando o erro do corretor turbinado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oe1L1gACwOjY"
      },
      "source": [
        "## Resumo e documentação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVE1FTZfwOjZ"
      },
      "source": [
        "### Resumo da aula 08"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lIl0fqNwOja"
      },
      "source": [
        "* No fim das contas, o corretor antigo é melhor do que o novo\n",
        "* Boa parte dos erros da base de testes necessita apenas de uma passagem pela cadeia de funções; passar duas vezes gera uma maior quantidade de erros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AI1R4dPjwOja"
      },
      "source": [
        "### Documentação da aula 08"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHYA7g1WwOjb"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcYHPaTYCWM0"
      },
      "source": [
        "## 1) Avaliando o resultado dos dois corretores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXI-flUhdXy9"
      },
      "source": [
        "Neste passo, testaremos o novo_corretor() e o compararemos com o corretor antigo. Verificando as qualidades de cada um, escolheremos o melhor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDxwGzOvCWM1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37929a41-7ab4-460c-faa4-0ea2fc51a4c9"
      },
      "source": [
        "def avaliador(testes, vocabulario):\n",
        "  n_palavras = len(testes)                                                      ## total de palavras avaliadas, contidas na base de dados de teste\n",
        "  acertos = 0                                                                   ## contador para palavras que o corretor acertou\n",
        "  desconhecida = 0                                                              ## contador para palavras desconhecidas\n",
        "  for correta,errada in testes:                                                 ## para cada tupla contendo um par de palavras (correta,errada) dentro da lista, faça o seguinte:\n",
        "    desconhecida += (correta not in vocabulario)\n",
        "    if novo_corretor(errada) == correta:                                        ## se a palavra errada ficar correta usando o corretor,\n",
        "      acertos += 1                                                              ## some 1 no contador\n",
        "    else:\n",
        "      print(errada + \"-\" + corretor(errada) + \"-\" + novo_corretor(errada))\n",
        "  taxa_acerto = acertos/n_palavras                                                             ## cálculo da taxa de acertos = acertos/total de palavras\n",
        "  taxa_desconhecida = desconhecida/n_palavras                                                  ## cálculo da taxa de desconhecimento = desconehcida/total de palavras\n",
        "  print(f'Taxa de acerto: {taxa_acerto*100:.2f}% de {n_palavras} palavras')                    ## qual foi a taxa de acertos?\n",
        "  print(f'Taxa de desconhecimento: {taxa_desconhecida*100:.2f}% de {n_palavras} palavras')     ## qual foi a taxa de desconhecimento das palavras?\n",
        "\n",
        "vocabulario = set(lista_normalizada)                                            ## todas as palavras do nosso vocabulário\n",
        "\n",
        "avaliador(lista_teste, vocabulario)                                             ## avaliador com dois parâmetros\n",
        "\n",
        "## o novo corretor é pior que o antigo, pois boa parte dos erros de digitação são corretamente corrigidos com apenas uma passagem pela cadeia de funções\n",
        "## passar duas vezes gerará novas palavras e aumenta a chance de erro"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "esje-esse-se\n",
            "sãêo-são-não\n",
            "dosa-dos-do\n",
            "eme-em-de\n",
            "eàssa-essa-esse\n",
            "daõs-das-da\n",
            "céda-cada-da\n",
            "noâ-no-o\n",
            "enêão-então-não\n",
            "tĩem-tem-em\n",
            "nossah-nossa-nosso\n",
            "teb-tem-de\n",
            "atĩ-até-a\n",
            "âem-em-de\n",
            "foo-foi-o\n",
            "serr-ser-se\n",
            "entke-entre-então\n",
            "van-vai-a\n",
            "çeus-seus-seu\n",
            "eû-e-de\n",
            "temeo-tempo-temos\n",
            "semre-sempre-ser\n",
            "elaá-ela-ele\n",
            "síó-só-se\n",
            "siàe-site-se\n",
            "seém-sem-em\n",
            "peln-pelo-ele\n",
            "aléra-alura-agora\n",
            "tdia-dia-da\n",
            "jé-é-de\n",
            "sãô-são-não\n",
            "odos-dos-do\n",
            "siua-sua-seu\n",
            "elpe-ele-esse\n",
            "teos-temos-os\n",
            "eũsa-essa-esse\n",
            "vjmos-vamos-temos\n",
            "dms-dos-de\n",
            "cava-java-para\n",
            "ános-nos-no\n",
            "èaso-caso-as\n",
            "túem-tem-em\n",
            "daáos-dados-dos\n",
            "nossk-nosso-nosso\n",
            "tãer-ter-ser\n",
            "vté-até-é\n",
            "búm-bem-um\n",
            "sçerá-será-ser\n",
            "entró-entre-então\n",
            "uai-vai-a\n",
            "sâus-seus-seu\n",
            "ìeu-seu-de\n",
            "fual-qual-sua\n",
            "elal-ela-ele\n",
            "skó-só-se\n",
            "secm-sem-em\n",
            "aluéa-alura-além\n",
            "dil-dia-de\n",
            "sód-só-se\n",
            "eúaa-aeúaa-essa\n",
            "ró-só-de\n",
            "dĩaz-adĩaz-da\n",
            "correptor-corretor-correto\n",
            "trtica-tática-prática\n",
            "ewpoderamento-aewpoderamento-ewpoderamento\n",
            "îgato-gato-fato\n",
            "cakvalo-acakvalo-carvalho\n",
            "canelac-acanelac-janela\n",
            "tênisy-atênisy-tênisy\n",
            "anciosa-aanciosa-ansioso\n",
            "ancciosa-aancciosa-ancciosa\n",
            "ansioa-aansioa-ensina\n",
            "asterístico-aasterístico-asterístico\n",
            "entertido-aentertido-entendido\n",
            "ritimo-ritmo-ótimo\n",
            "indiota-aindiota-indica\n",
            "tomare-tomar-tomar\n",
            "seje-seja-se\n",
            "provalecer-aprovalecer-prevalece\n",
            "esteje-esteja-este\n",
            "mindigo-amindigo-indico\n",
            "pertubar-apertubar-derrubar\n",
            "Taxa de acerto: 55.91% de 186 palavras\n",
            "Taxa de desconhecimento: 6.99% de 186 palavras\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uPwWMhMCWM1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea17c4b1-3612-4d42-f3da-cdb2d2cbce67"
      },
      "source": [
        "## esquece o novo corretor\n",
        "\n",
        "def avaliador(testes, vocabulario):\n",
        "  n_palavras = len(testes)                                                      ## total de palavras avaliadas, contidas na base de dados de teste\n",
        "  acertos = 0                                                                   ## contador para palavras que o corretor acertou\n",
        "  desconhecida = 0                                                              ## contador para palavras desconhecidas\n",
        "  for correta,errada in testes:                                                 ## para cada tupla contendo um par de palavras (correta,errada) dentro da lista, faça o seguinte:\n",
        "    desconhecida += (correta not in vocabulario)\n",
        "    if corretor(errada) == correta:                                             ## se a palavra errada ficar correta usando o corretor,\n",
        "      acertos += 1                                                              ## some 1 no contador\n",
        "  taxa_acerto = acertos/n_palavras                                                             ## cálculo da taxa de acertos = acertos/total de palavras\n",
        "  taxa_desconhecida = desconhecida/n_palavras                                                  ## cálculo da taxa de desconhecimento = desconehcida/total de palavras\n",
        "  print(f'Taxa de acerto: {taxa_acerto*100:.2f}% de {n_palavras} palavras')                    ## qual foi a taxa de acertos?\n",
        "  print(f'Taxa de desconhecimento: {taxa_desconhecida*100:.2f}% de {n_palavras} palavras')     ## qual foi a taxa de desconhecimento das palavras?\n",
        "\n",
        "vocabulario = set(lista_normalizada)                                            ## todas as palavras do nosso vocabulário\n",
        "\n",
        "avaliador(lista_teste, vocabulario)                                             ## avaliador com dois parâmetros"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Taxa de acerto: 76.34% de 186 palavras\n",
            "Taxa de desconhecimento: 6.99% de 186 palavras\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c82L5ycEghxP",
        "outputId": "1dfb133a-9330-48dd-9e34-8eb612101f10"
      },
      "source": [
        "palavra = \"lóiigica\"\n",
        "print(novo_corretor(palavra))\n",
        "print(corretor(palavra))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lógica\n",
            "alóiigica\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rxdVXnTje0A"
      },
      "source": [
        "# <font color=navyblue> `Documentação`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcT7I5MV8azZ"
      },
      "source": [
        "## AULA 01\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qquVdDlW9b-9"
      },
      "source": [
        "1)\n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "\n",
        "\n",
        "2)\n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "\n",
        "\n",
        "3)\n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "* "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lIZ9qHw9hnG"
      },
      "source": [
        "## AULA 02\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UO_faHGo9hnL"
      },
      "source": [
        "1)\n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "\n",
        "\n",
        "2)\n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "\n",
        "\n",
        "3)\n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "* "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ur_FLKmb9inm"
      },
      "source": [
        "## AULA 03\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRsYiZwe9ino"
      },
      "source": [
        "1)\n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "\n",
        "\n",
        "2)\n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "\n",
        "\n",
        "3)\n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "* "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R47ZabiT9jG_"
      },
      "source": [
        "## AULA 04\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVOhhO9p9jHA"
      },
      "source": [
        "1)\n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "\n",
        "\n",
        "2)\n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "\n",
        "\n",
        "3)\n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "* "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bV8QDcbN9jbY"
      },
      "source": [
        "## AULA 05\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIt2__Ce9jbZ"
      },
      "source": [
        "1)\n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "\n",
        "\n",
        "2)\n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "\n",
        "\n",
        "3)\n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "* "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-m582bq19jst"
      },
      "source": [
        "## AULA 06\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZIAC9829jsu"
      },
      "source": [
        "1)\n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "\n",
        "\n",
        "2)\n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "\n",
        "\n",
        "3)\n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "* "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4cidb2W9j-9"
      },
      "source": [
        "## AULA 07\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roTDrOVw9j-9"
      },
      "source": [
        "1)\n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "\n",
        "\n",
        "2)\n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "\n",
        "\n",
        "3)\n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "* "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WQpQFoV9kQn"
      },
      "source": [
        "## AULA 08\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKQwcrFn9kQn"
      },
      "source": [
        "1)\n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "\n",
        "\n",
        "2)\n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "\n",
        "\n",
        "3)\n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "* \n",
        "* "
      ]
    }
  ]
}